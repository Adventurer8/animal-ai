diff --git a/animalai/animalai.egg-info/PKG-INFO b/animalai/animalai.egg-info/PKG-INFO
index 2f3704b..ef94781 100644
--- a/animalai/animalai.egg-info/PKG-INFO
+++ b/animalai/animalai.egg-info/PKG-INFO
@@ -5,6 +5,8 @@ Summary: Animal AI environment Python API
 Home-page: https://github.com/mdcrosby/animal-ai
 Author: Matt Crosby
 Author-email: matt@mdcrosby.com
+License: UNKNOWN
+Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: License :: OSI Approved :: Apache Software License
@@ -12,3 +14,6 @@ Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Requires-Python: >=3.6
+
+UNKNOWN
+
diff --git a/animalai/animalai.egg-info/SOURCES.txt b/animalai/animalai.egg-info/SOURCES.txt
index ee63c1f..9b3dd91 100644
--- a/animalai/animalai.egg-info/SOURCES.txt
+++ b/animalai/animalai.egg-info/SOURCES.txt
@@ -1,7 +1,18 @@
 setup.py
+animalai/__init__.py
 animalai.egg-info/PKG-INFO
 animalai.egg-info/SOURCES.txt
 animalai.egg-info/dependency_links.txt
 animalai.egg-info/not-zip-safe
 animalai.egg-info/requires.txt
-animalai.egg-info/top_level.txt
\ No newline at end of file
+animalai.egg-info/top_level.txt
+animalai/envs/__init__.py
+animalai/envs/actions.py
+animalai/envs/braitenberg.py
+animalai/envs/environment.py
+animalai/envs/raycastparser.py
+animalai/envs/settings.py
+animalai/envs/gym/__init__.py
+animalai/envs/gym/environment.py
+animalai/train/__init__.py
+animalai/train/train.py
\ No newline at end of file
diff --git a/animalai/animalai.egg-info/top_level.txt b/animalai/animalai.egg-info/top_level.txt
index 8b13789..1b03e73 100644
--- a/animalai/animalai.egg-info/top_level.txt
+++ b/animalai/animalai.egg-info/top_level.txt
@@ -1 +1 @@
-
+animalai
diff --git a/examples/gymwrapper.py b/examples/gymwrapper.py
index 3c5a0ae..be15193 100644
--- a/examples/gymwrapper.py
+++ b/examples/gymwrapper.py
@@ -1,19 +1,97 @@
 # from stable_baselines3 import PPO
 from stable_baselines3 import DQN
 import torch as th
+import argparse
+from distutils.util import strtobool
 
 import sys
 import random
 import os
+import time
 # from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv
 from gym_unity.envs import UnityToGymWrapper
 from animalai.envs.environment import AnimalAIEnvironment
 
+
+def parse_args():
+    # fmt: off
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
+        help="the name of this experiment")
+    parser.add_argument("--seed", type=int, default=1,
+        help="seed of the experiment")
+    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, `torch.backends.cudnn.deterministic=False`")
+    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, cuda will be enabled by default")
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, this experiment will be tracked with Weights and Biases")
+    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+        help="the wandb's project name")
+    parser.add_argument("--wandb-entity", type=str, default=None,
+        help="the entity (team) of wandb's project")
+    parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="whether to capture videos of the agent performances (check out `videos` folder)")
+    parser.add_argument("--save-model", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="whether to save model into the `runs/{run_name}` folder")
+    parser.add_argument("--upload-model", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="whether to upload the saved model to huggingface")
+    parser.add_argument("--hf-entity", type=str, default="",
+        help="the user or org name of the model repository from the Hugging Face Hub")
+
+    # Algorithm specific arguments
+    parser.add_argument("--env-id", type=str, default="CartPole-v1",
+        help="the id of the environment")
+    parser.add_argument("--total-timesteps", type=int, default=500000,
+        help="total timesteps of the experiments")
+    parser.add_argument("--learning-rate", type=float, default=2.5e-4,
+        help="the learning rate of the optimizer")
+    parser.add_argument("--num-envs", type=int, default=1,
+        help="the number of parallel game environments")
+    parser.add_argument("--buffer-size", type=int, default=10000,
+        help="the replay memory buffer size")
+    parser.add_argument("--gamma", type=float, default=0.99,
+        help="the discount factor gamma")
+    parser.add_argument("--tau", type=float, default=1.,
+        help="the target network update rate")
+    parser.add_argument("--target-network-frequency", type=int, default=500,
+        help="the timesteps it takes to update the target network")
+    parser.add_argument("--batch-size", type=int, default=128,
+        help="the batch size of sample from the reply memory")
+    parser.add_argument("--start-e", type=float, default=1,
+        help="the starting epsilon for exploration")
+    parser.add_argument("--end-e", type=float, default=0.05,
+        help="the ending epsilon for exploration")
+    parser.add_argument("--exploration-fraction", type=float, default=0.5,
+        help="the fraction of `total-timesteps` it takes from start-e to go end-e")
+    parser.add_argument("--learning-starts", type=int, default=10000,
+        help="timestep to start learning")
+    parser.add_argument("--train-frequency", type=int, default=10,
+        help="the frequency of training")
+    args = parser.parse_args()
+    # fmt: on
+    assert args.num_envs == 1, "vectorized envs are not supported at the moment"
+
+    return args
+
 def train_agent_single_config(configuration_file):
 
+    args =  parse_args()
+    import wandb
+
+    wandb.init(
+        project='dqn',
+        entity=None,
+        sync_tensorboard=True,
+        config=vars(args),
+        name=f"{int(time.time())}",
+        monitor_gym=True,
+        save_code=True,
+    )
+
     aai_env = AnimalAIEnvironment(
         seed = 123,
-        file_name="../env/AnimalAI",
+        file_name="/home/roma/animal-ai/env/AAI_v3.0.1_build_linux_090422.x86_64",
         arenas_configurations=configuration_file,
         play=False,
         base_port=5000,
@@ -38,8 +116,8 @@ def train_agent_single_config(configuration_file):
     policy_kwargs = dict(activation_fn=th.nn.ReLU)
     model = DQN("CnnPolicy", env, policy_kwargs=policy_kwargs, verbose=1, tensorboard_log=("./dqn_tensorboard/" + runname))
 
-    no_saves = 100
-    no_steps = 1000000
+    no_saves = 10
+    no_steps = 1000
     reset_num_timesteps = True
     for i in range(no_saves):
         model.learn(no_steps, reset_num_timesteps=reset_num_timesteps)
@@ -51,11 +129,14 @@ def train_agent_single_config(configuration_file):
 if __name__ == "__main__":
     if len(sys.argv) > 1:
         configuration_file = sys.argv[1]
-    else:   
-        competition_folder = "../configs/competition/"
-        configuration_files = os.listdir(competition_folder)
+    else:
+        conf = ['01-01-01', '01-01-02', '01-01-03', '01-02-01', '01-02-02', '01-02-03', '01-03-01', '01-03-02', '1-03-03']
+
+        competition_folder = "/home/roma/animal-ai/configs/competition/"
+        configuration_files = conf #os.listdir(competition_folder)
         configuration_random = random.randint(0, len(configuration_files))
         configuration_file = (
             competition_folder + configuration_files[configuration_random]
+            +'.yaml'
         )
     train_agent_single_config(configuration_file=configuration_file)
diff --git a/examples/lowlevelapi.py b/examples/lowlevelapi.py
index 0b1d8d5..daa862c 100644
--- a/examples/lowlevelapi.py
+++ b/examples/lowlevelapi.py
@@ -11,11 +11,11 @@ def run_agent_single_config(configuration_file: str) -> None:
     See https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Python-API.md for details.
     For demo purposes uses a simple braitenberg vehicle-inspired agent that solves most tasks from category 1.
     """
-    env_path = "../env/AnimalAI"
+    env_path = "/home/roma/animal-ai/env/AAI_v3.0.1_build_linux_090422.x86_64"
     
     configuration = configuration_file
 
-    totalRays = 9
+    totalRays = 7
     env = AnimalAIEnvironment(
         file_name=env_path,
         arenas_configurations=configuration,
@@ -63,7 +63,7 @@ if __name__ == "__main__":
     if len(sys.argv) > 1:
         configuration_file = sys.argv[1]
     else:
-        competition_folder = "../configs/competition/"
+        competition_folder = "/home/roma/animal-ai/configs/competition/"
         configuration_files = os.listdir(competition_folder)
         configuration_random = random.randint(0, len(configuration_files))
         configuration_file = (
diff --git a/examples/play.py b/examples/play.py
index a0fece2..74dd19a 100644
--- a/examples/play.py
+++ b/examples/play.py
@@ -10,7 +10,7 @@ def load_config_and_play(configuration_file: str) -> None:
     :param configuration_file: str path to the yaml configuration
     :return: None
     """
-    env_path = "../env/AnimalAI"
+    env_path = "../env/AAI_v3.0.1_build_linux_090422.x86_64"
     port = 5005 + random.randint(
         0, 1000
     )  # use a random port to avoid problems if a previous version exits slowly
@@ -39,10 +39,16 @@ if __name__ == "__main__":
     if len(sys.argv) > 1:
         configuration_file = sys.argv[1]
     else:
+        # Introductory experiments - Basic Food Variations
+        conf = ['01-01-01', '01-01-02', '01-01-03', '01-02-01', '01-02-02', '01-02-03', '01-03-01', '01-03-02', '1-03-03']
+        
         competition_folder = "../configs/competition/"
         configuration_files = os.listdir(competition_folder)
-        configuration_random = random.randint(0, len(configuration_files))
+        # configuration_files = conf
+
+        configuration_random = random.randint(0, len(configuration_files)-1)
         configuration_file = competition_folder + configuration_files[configuration_random]
+        # configuration_file = competition_folder + configuration_files[configuration_random] + '.yaml'
         print(F"Using configuration file {configuration_file}")
     
     load_config_and_play(configuration_file=configuration_file)
\ No newline at end of file
diff --git a/examples/trainaai.py b/examples/trainaai.py
index c559321..21a3fe2 100644
--- a/examples/trainaai.py
+++ b/examples/trainaai.py
@@ -9,7 +9,7 @@ if __name__ == "__main__":
         print("  This script lets you use AnimalAI with mlagents-learn")
         print("  The first argument should be the relative path to your AnimalAI training config.yml")
         print("  The following arguments should be the same as for using mlagents-learn directly")
-        print(" e.g. python trainaai.py training_configs/aai_tinyrays.yml training_configs/ppo_tiny.yaml --env=../env/AnimalAI --run-id=foo")
+        print(" e.g. python3 trainaai.py training_configs/aai_tinyrays.yml training_configs/ppo_tiny.yaml --env=../env/AAI_v3.0.1_build_linux_090422.x86_64 --run-id=foo")
         sys.exit()
 
     aai_opt = AAIOptions.load_config(sys.argv[1])
